---
title: "p8105_hw3_gwk2111"
author: "Zach Keefer"
date: "10/11/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(p8105.datasets)
```

##Problem 1

Cleaning the data in order to answer the subsequent questions:
```{r}
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE)) 
```

###In 2002, which states were observed at 7 locations?
```{r}
filter(brfss_data, year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarise(location_freq = n_distinct(geo_location)) %>% 
  filter(location_freq == 7)
```

In 2002, Connecticut, Florida, and North Carolina were observed at 7 locations.


###Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010:
```{r}
group_by(brfss_data, locationabbr, year) %>% 
  summarise(location_freq = n_distinct(geo_location)) %>%  
   ggplot(aes(x = year, y = location_freq)) +
       geom_line(aes(color = locationabbr)) +
  labs(x = "Year",
       y = "Location Frequency",
       color = "State")
```


###Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State:
```{r}
filter(brfss_data, year == 2002 | year == 2006 | year == 2010, locationabbr == "NY") %>% 
  spread(key = response, value = data_value) %>% 
  group_by(year) %>% 
  summarize(mean_proportion = mean(Excellent, na.rm = TRUE),
            sd = sd(Excellent, na.rm = TRUE)) %>% 
  knitr::kable(digits = 3)
```


###For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time:
```{r}
spread(brfss_data, key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  group_by(locationabbr, year) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
         mean_very_good = mean(very_good, na.rm = TRUE),
         mean_good = mean(good, na.rm = TRUE),
         mean_fair = mean(fair, na.rm = TRUE),
         mean_poor = mean(poor, na.rm = TRUE)) %>%
  gather(key = response, value = mean, mean_excellent:mean_poor) %>% 
  ggplot(aes(x = year, y = mean)) +
  geom_line(aes(color = locationabbr)) +
  facet_grid(~response) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(y = "Mean",
       x = "Year",
       color = "State")
```


##Problem 2

Cleaning the data in order to answer the subsequent questions:
```{r}
instacart = instacart %>% 
  janitor::clean_names()
```

The following is a data set from Instacart, an online grocery service.  It is a collection of over 1 million orders from more than 100,000 Instacart users.  There are 1,384,617 obersavtions in the data set, with each observation representing a product from an order, which means one order can produce multiple observations.  Furthremore, there are 15 variables in the dataset  Of particular interest in this data set are `product_name` (character variable) and `product_id` (numeric variable) which describe the product being ordered, `aisle` (character variable) and `aisle_id` (numeric variable) which describe the aisle that the product is being ordered from, and `order_dow` (numeric variable) and `order_hour_of_day` (numeric variable) which describe the day and time the products were ordered.  Any single observation in the dataset will tell us what order the product was from, the day of the week and time it was ordered, the user id of who ordered the product, the product ordered, the aisle the product was ordered from and the department the aisle was in.


###How many aisles are there, and which aisles are the most items ordered from?
```{r}
count(instacart, aisle_id) %>% 
  nrow()

count(instacart, aisle) %>% 
  filter(min_rank(desc(n)) < 5)
```

There are 134 total aisles.  The two aisles that are the most ordered from are Fresh Fruits (n = 150,473) and Fresh Vegetables (n = 150,609).


###Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
```{r}
group_by(instacart, aisle_id) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = aisle_id, y = n)) +
  geom_bar(stat = "identity") +
  ##Decided to make breaks at every 10.  Because there are 134 observations, I made a break at 127 and 134 at the end for readability purposes
  scale_x_continuous(breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 127, 134),
                     labels = c("0", "10", "20", "30", "40", "50", "60", "70", "80", "90", "100", "110", "120", "127", "134")) +
  labs(x = "Aisle Number",
       y = "Count")
```

Plot is ordered by `aisle_id` (1 to 134).


###Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
```{r}
filter(instacart, aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(most_popular = n()) %>%
  filter(min_rank(desc(most_popular)) < 2) %>% 
  knitr::kable()
```


###Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r}
library(lubridate)
filter(instacart, product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  mutate(order_dow = order_dow + 1) %>% 
  mutate(day = wday(order_dow, label = TRUE)) %>% 
  group_by(day, product_name) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(mean_hour = hour(date_decimal(mean_hour)),
         ##Changing from military time and adding am/pm
         mean_hour = format(strptime(mean_hour, format = "%H"), "%I %p")) %>% 
  spread(key = day, value = mean_hour) %>% 
  knitr::kable()
  
```


##Problem 3

Cleaning the data in order to answer the subsequent questions:
```{r}
ny_noaa = ny_noaa %>% 
  janitor::clean_names()
```

The following dataset is pulled from the GHCN (Global Historical Climatology Network)-Daily database, which collects summary statistics from weather stations around the world.  This particular dataset contains five core variables (precipitation, snowfall, snow depth, and minimum and maximum temperature) from all New York state weather stations from January 1, 1981 through December 31, 2010.  The dataset has 2,595,176 observations and 7 variables (I will add 3 more variables below).  The `id` variable is the unique identifier for each weather station, and `date` describes the date of the observation.  `Prcp` is precipitation and is measured in tenths of mm, while `snow` (snowfall) and `snwd` (snowdepth) are measured in mm.  Finally, `tmax` (maximum temperature) and `tmin` (minimum temperature) are measured in tenths of degress celcius and are character variables.  There are a number of missing values in this dataset.  There are `r sum(is.na(ny_noaa$prcp))` missing `prcp` values, `r sum(is.na(ny_noaa$snow))` missing `snow` values, `r sum(is.na(ny_noaa$snwd))` missing `snwd` values, `r sum(is.na(ny_noaa$tmax))` missing `tmax` values, and `r sum(is.na(ny_noaa$tmin))`
missing `tmin` values. The extent to which missing data is an issue depends on whether the data is missing at random.  If the values are not missing at random, than it can be a problem and may produce bias in our analyses.

###Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
```{r}
ny_noaa = ny_noaa %>% 
  mutate(year = year(date),
       month = month(date, label = TRUE),
       day = lubridate::floor_date(date, unit = "day"),
       ##Precipitation is originally given in tenths of mm, so divide by 10 to make it more familiar
       prcp = prcp/10,
       ##Convert tmin to numeric, and then divide by 10 to make into degrees.
       tmin = (as.numeric(tmin))/10,
       ##Convert tmax to numeric, and then divide by 10 to make into degrees.
       tmax = (as.numeric(tmax))/10) 
 
count(ny_noaa, snow) %>%
  filter(min_rank(desc(n)) < 3) 

```

For snowfall, the most commonly observed variables are `0` (n = 2,008,508) and `NA` (n = 381,221).  Since observations are daily, and the majority of days in New York State there is no snowfall, we would expect to see `0` as the most obvserved variable.  Furthermore, there is a high number of missing observations throughout the data, so it is not surprising that `NA` is the second more observed variable.


###Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r}
filter(ny_noaa, month == "Jan" | month == "Jul") %>% 
  group_by(month, id, year) %>% 
  summarize(mean_temp = mean(tmax, na.rm = FALSE)) %>% 
  ggplot(aes(x = year, y = mean_temp)) +
  geom_point() +
  geom_smooth() +
  facet_grid(~month) +
  labs(x = "Year",
       y = "Mean Temperature")
```

Based on the plots, there seems to be more variation in average max temperature in January than there is July.  There are a few outlier station temperatures, especially one particularly low temperature in July, 1988.  In addition, January, 1994 was a bit of an outlier in terms of average temperature and seems to have contributed to the variance of January temperatures.  


###Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}
library(hexbin)
library(ggridges)
library(patchwork)
tmax_tmin_p = ggplot(ny_noaa, aes(x = tmin, y = tmax)) +
  ##With the high number of points, used geom_hex to avoid overplotting.
  geom_hex() 

snowfall_p = filter(ny_noaa, snow > 0 & snow < 100) %>%
  mutate(year = as.character(year)) %>% 
  ggplot(aes(x = snow, y = year)) + 
  ##Used geom_density because we have a lot of categories where the shape of the distribution matters.
  geom_density_ridges() +
  labs(x = "Snowfall (mm)",
       y = "Year")

(tmax_tmin_p + snowfall_p)


```

